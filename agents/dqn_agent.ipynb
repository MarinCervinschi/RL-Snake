{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) Training Notebook\n",
    "\n",
    "This notebook trains a **DQN with CNN** agent on the Snake game.\n",
    "\n",
    "**Grid Size:** 10Ã—10 (optimal for DQN)\n",
    "\n",
    "**Expected Performance:**\n",
    "\n",
    "- Episodes to convergence: 2000-3000\n",
    "- Final average score: 15-25 apples\n",
    "- Training time: 15-20 minutes (CPU) / 5-10 minutes (GPU)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# !git clone https://github.com/MarinCervinschi/rl-snake.git\n",
    "# %cd rl-snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import random\n",
    "from typing import List, Deque\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.config import GameConfig\n",
    "from game.engine import SnakeGameEngine\n",
    "from game.entities import Action, State\n",
    "from utils.metrics import TrainingMetrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DQN Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Q-Network for processing grid states.\n",
    "\n",
    "    Input: (3, H, W)\n",
    "    Output: Q-values for each action\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size: int, num_actions: int = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.flat_size = 64 * grid_size * grid_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        features = self.conv(x)\n",
    "        flat = features.view(features.size(0), -1)\n",
    "        q_values = self.fc(flat)\n",
    "        return q_values\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 100_000):\n",
    "        self.buffer: Deque = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQN Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int = 10,\n",
    "        learning_rate: float = 0.0001,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.9995,\n",
    "        learning_starts: int = 2000,\n",
    "        min_epsilon: float = 0.05,\n",
    "        batch_size: int = 64,\n",
    "        buffer_size: int = 100_000,\n",
    "        target_update_freq: int = 1000,\n",
    "        continue_training: bool = False,\n",
    "    ):\n",
    "        self.grid_size = grid_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_starts = learning_starts\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        # Networks\n",
    "        self.q_network = ConvQNetwork(grid_size).to(device)\n",
    "        self.target_network = ConvQNetwork(grid_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Training statistics\n",
    "        self.steps = 0\n",
    "        self.episodes_trained = 0\n",
    "        self.losses: List[float] = []\n",
    "        self.episode_rewards: List[float] = []\n",
    "\n",
    "        if continue_training:\n",
    "            self.load()\n",
    "\n",
    "    def get_action(self, state: State, training: bool = True) -> Action:\n",
    "        \"\"\"\n",
    "        Select action with adaptive epsilon-greedy.\n",
    "\n",
    "        Args:\n",
    "            state: Current game state\n",
    "            training: If False, uses greedy policy (no exploration)\n",
    "        \"\"\"\n",
    "        # During warmup: pure exploration\n",
    "        if training and self.steps < self.learning_starts:\n",
    "            return random.choice(list(Action))\n",
    "\n",
    "        # Epsilon-greedy (only during training)\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(list(Action))\n",
    "\n",
    "        # Greedy action from Q-network\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        return Action(q_values.argmax().item())\n",
    "\n",
    "    def train(\n",
    "        self, state: State, action: Action, reward: float, next_state: State, done: bool\n",
    "    ):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        state_array = state.to_tensor()\n",
    "        next_state_array = next_state.to_tensor()\n",
    "\n",
    "        # Store in replay buffer\n",
    "        self.memory.push(state_array, action.value, reward, next_state_array, done)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # Don't train until warmup complete\n",
    "        if self.steps < self.learning_starts:\n",
    "            return\n",
    "\n",
    "        # Don't train if buffer too small\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Perform training update\n",
    "        loss = self._get_loss()\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Update target network\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        if done and self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.episodes_trained += 1\n",
    "\n",
    "    def _get_loss(self):\n",
    "        \"\"\"Compute loss for a training step.\"\"\"\n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(\n",
    "            self.batch_size\n",
    "        )\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_t = torch.as_tensor(states, device=device)\n",
    "        actions_t = torch.as_tensor(actions, device=device)\n",
    "        rewards_t = torch.as_tensor(rewards, device=device)\n",
    "        next_states_t = torch.as_tensor(next_states, device=device)\n",
    "        dones_t = torch.as_tensor(dones, device=device)\n",
    "\n",
    "        # Current Q-values\n",
    "        q_values = self.q_network(states_t)\n",
    "        current_q = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            # Double DQN: use online network to select, target to evaluate\n",
    "            next_actions = self.q_network(next_states_t).argmax(1)\n",
    "            next_q = (\n",
    "                self.target_network(next_states_t)\n",
    "                .gather(1, next_actions.unsqueeze(1))\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            # Bellman update\n",
    "            target_q = rewards_t + (1 - dones_t) * self.discount_factor * next_q\n",
    "\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _state_to_tensor(self, state: State) -> torch.Tensor:\n",
    "        \"\"\"Convert State to tensor.\"\"\"\n",
    "        state_array = state.to_tensor()\n",
    "        return torch.FloatTensor(state_array).unsqueeze(0).to(device)\n",
    "\n",
    "    def save(self, filepath: str = \"models/dqn_cnn.pkl\"):\n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"q_network_state\": self.q_network.state_dict(),\n",
    "                \"target_network_state\": self.target_network.state_dict(),\n",
    "                \"optimizer_state\": self.optimizer.state_dict(),\n",
    "                \"grid_size\": self.grid_size,\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"steps\": self.steps,\n",
    "                \"episodes_trained\": self.episodes_trained,\n",
    "            },\n",
    "            filepath,\n",
    "        )\n",
    "        print(f\"ðŸ’¾ Model saved to {filepath}\")\n",
    "        print(f\"   Episodes: {self.episodes_trained}\")\n",
    "\n",
    "    def load(self, filepath: str = \"models/dqn_cnn.pkl\"):\n",
    "        \"\"\"Load model and training state.\"\"\"\n",
    "        if not Path(filepath).exists():\n",
    "            print(f\"âš ï¸  No saved model found at {filepath}\")\n",
    "            return\n",
    "\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "        self.q_network.load_state_dict(checkpoint[\"q_network_state\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network_state\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        self.grid_size = checkpoint[\"grid_size\"]\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "        self.steps = checkpoint[\"steps\"]\n",
    "        self.episodes_trained = checkpoint[\"episodes_trained\"]\n",
    "\n",
    "        print(f\"âœ… Model loaded from {filepath}\")\n",
    "        print(f\"   Episodes: {self.episodes_trained}\")\n",
    "        print(f\"   Steps: {self.steps:,}\")\n",
    "        print(f\"   Epsilon: {self.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "GRID_SIZE = 10\n",
    "EPISODES = 6_000\n",
    "\n",
    "# Agent hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.9995\n",
    "LEARNING_STARTS = 2_000\n",
    "EPSILON_MIN = 0.05\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100_000\n",
    "TARGET_UPDATE = 1_000\n",
    "\n",
    "CONTINUE_TRAINING = False\n",
    "\n",
    "# Save locations\n",
    "MODEL_PATH = \"models/dqn_cnn.pkl\"\n",
    "RESULTS_DIR = f\"results/dqn_{GRID_SIZE}x{GRID_SIZE}\"\n",
    "\n",
    "print(f\"ðŸŽ® Training DQN on {GRID_SIZE}Ã—{GRID_SIZE} grid\")\n",
    "print(f\"ðŸ“ˆ Episodes: {EPISODES:,}\")\n",
    "print(f\"ðŸ–¥ï¸  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Environment and Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "config = GameConfig(grid_size=GRID_SIZE)\n",
    "game = SnakeGameEngine(config)\n",
    "\n",
    "# Agent\n",
    "agent = DQNAgent(\n",
    "    grid_size=GRID_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    epsilon=EPSILON_START,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    learning_starts=LEARNING_STARTS,\n",
    "    min_epsilon=EPSILON_MIN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    target_update_freq=TARGET_UPDATE,\n",
    "    continue_training=CONTINUE_TRAINING,\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "metrics = TrainingMetrics(save_dir=RESULTS_DIR)\n",
    "\n",
    "print(\"âœ… Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score = 0\n",
    "pbar = tqdm(range(1, EPISODES + 1), desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    for episode in pbar:\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            reward, done, score = game.step(action)\n",
    "            next_state = game.get_state()\n",
    "\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        metrics.record_episode(episode, score, steps, episode_reward)\n",
    "\n",
    "        if score > record_score:\n",
    "            record_score = score\n",
    "\n",
    "        # Update progress\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Avg\": f\"{metrics.get_recent_average_score():.2f}\",\n",
    "                \"Best\": record_score,\n",
    "                \"Îµ\": f\"{agent.epsilon:.3f}\",\n",
    "                \"Buffer\": len(agent.memory),\n",
    "            }\n",
    "        )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nâš ï¸  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nâŒ An error occurred: {e}\")\n",
    "    raise e\n",
    "else:\n",
    "    print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot(show=True, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loss Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent.losses:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Raw loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(agent.losses, alpha=0.3)\n",
    "    if len(agent.losses) > 1000:\n",
    "        window = 1000\n",
    "        smoothed = np.convolve(agent.losses, np.ones(window) / window, mode=\"valid\")\n",
    "        plt.plot(smoothed, linewidth=2, label=f\"Moving Avg ({window})\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(agent.losses, bins=50, alpha=0.7)\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Loss Distribution\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Loss Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(agent.losses):.4f}\")\n",
    "    print(f\"  Std: {np.std(agent.losses):.4f}\")\n",
    "    print(f\"  Min: {np.min(agent.losses):.4f}\")\n",
    "    print(f\"  Max: {np.max(agent.losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Trained Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0.0  # Pure exploitation\n",
    "\n",
    "test_episodes = 10\n",
    "test_scores = []\n",
    "\n",
    "print(f\"Testing for {test_episodes} episodes...\")\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = game.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = game.step(action)\n",
    "        state = game.get_state()\n",
    "\n",
    "    test_scores.append(score)\n",
    "    print(f\"  Episode {ep+1}: Score = {score}\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Average: {np.mean(test_scores):.2f}\")\n",
    "print(f\"  Best: {max(test_scores)}\")\n",
    "print(f\"  Worst: {min(test_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export for Google Colab (Optional)\n",
    "\n",
    "If running on Google Colab, you can download the trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download model in Colab\n",
    "# from google.colab import files\n",
    "# files.download(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "rl-snake (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
