{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) Training Notebook\n",
    "\n",
    "This notebook trains a **DQN with CNN** agent on the Snake game.\n",
    "\n",
    "**Grid Size:** 10√ó10 (optimal for DQN)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Episodes to convergence: 2000-3000\n",
    "- Final average score: 15-25 apples\n",
    "- Training time: 15-20 minutes (CPU) / 5-10 minutes (GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.config import GameConfig\n",
    "from game.engine import SnakeGameEngine\n",
    "from game.entities import Action, State\n",
    "from utils.metrics import TrainingMetrics\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# For Google Colab\n",
    "# !git clone https://github.com/YOUR_REPO/rl-snake.git\n",
    "# %cd rl-snake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DQN Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Q-Network optimized for Snake.\n",
    "\n",
    "    Architecture choices:\n",
    "    - ReLU: Standard, fast, no saturation issues\n",
    "    - Batch Normalization: Stabilizes training\n",
    "    - Residual connections: Better gradient flow\n",
    "    - Adaptive architecture: Scales with grid size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size: int, num_actions: int = 4):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        if grid_size <= 10:\n",
    "            channels = [32, 64, 64]\n",
    "            hidden_size = 256\n",
    "        elif grid_size <= 15:\n",
    "            channels = [32, 64, 128]\n",
    "            hidden_size = 512\n",
    "        else:\n",
    "            channels = [32, 64, 128, 128]\n",
    "            hidden_size = 512\n",
    "\n",
    "        conv_layers = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for out_channels in channels:\n",
    "            conv_layers.extend(\n",
    "                [\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(out_channels),  # Stabilizes training\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ]\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        self.flat_size = channels[-1] * grid_size * grid_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),  # Prevent overfitting\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size // 2, num_actions),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.conv(x)\n",
    "        flat = features.view(features.size(0), -1)\n",
    "        return self.fc(flat)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Kaiming initialization for ReLU networks.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized Experience Replay.\n",
    "\n",
    "    Samples important transitions more frequently.\n",
    "    Improves learning efficiency significantly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int = 100_000, alpha: float = 0.6, beta: float = 0.4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Prioritization exponent\n",
    "        self.beta = beta  # Importance sampling exponent\n",
    "        self.beta_increment = 0.001\n",
    "\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition with max priority.\"\"\"\n",
    "        max_priority = self.priorities.max() if self.size > 0 else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample batch with prioritization.\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            return None\n",
    "\n",
    "        # Calculate sampling probabilities\n",
    "        priorities = self.priorities[: self.size]\n",
    "        probs = priorities**self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # Sample indices\n",
    "        indices = np.random.choice(self.size, batch_size, p=probs, replace=False)\n",
    "\n",
    "        # Calculate importance sampling weights\n",
    "        weights = (self.size * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()  # Normalize\n",
    "\n",
    "        # Get samples\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Increment beta (anneal importance sampling)\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "            indices,\n",
    "            weights,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors.\"\"\"\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = abs(error) + 1e-6  # Small constant for stability\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int,\n",
    "        learning_rate: float = 0.0001,\n",
    "        discount_factor: float = 0.99,\n",
    "        # Exploration parameters (adaptive)\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.05,  # Keep some exploration\n",
    "        epsilon_frames: int = 500_000,  # Decay over more frames\n",
    "        # Training parameters\n",
    "        batch_size: int = 64,\n",
    "        buffer_size: int = 100_000,\n",
    "        target_update_freq: int = 2_000,  # Less frequent updates\n",
    "        learning_starts: int = 10_000,  # Warmup period\n",
    "    ):\n",
    "        self.grid_size = grid_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learning_starts = learning_starts\n",
    "\n",
    "        # Adaptive epsilon decay\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_frames = epsilon_frames\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_frames\n",
    "\n",
    "        # Networks\n",
    "        self.q_network = ConvQNetwork(grid_size).to(device)\n",
    "        self.target_network = ConvQNetwork(grid_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Optimizer with weight decay (L2 regularization)\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.q_network.parameters(), lr=learning_rate, weight_decay=1e-5\n",
    "        )\n",
    "\n",
    "        # Huber loss (robust to outliers)\n",
    "        self.criterion = nn.SmoothL1Loss(reduction=\"none\")  # Per-sample loss for PER\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "\n",
    "        # Training statistics\n",
    "        self.steps = 0\n",
    "        self.episodes_trained = 0\n",
    "        self.losses: List[float] = []\n",
    "        self.episode_rewards: List[float] = []\n",
    "\n",
    "    def get_action(self, state: State, training: bool = True) -> Action:\n",
    "        \"\"\"\n",
    "        Select action with adaptive epsilon-greedy.\n",
    "\n",
    "        Args:\n",
    "            state: Current game state\n",
    "            training: If False, uses greedy policy (no exploration)\n",
    "        \"\"\"\n",
    "        # During warmup: pure exploration\n",
    "        if training and self.steps < self.learning_starts:\n",
    "            return random.choice(list(Action))\n",
    "\n",
    "        # Epsilon-greedy (only during training)\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(list(Action))\n",
    "\n",
    "        # Greedy action from Q-network\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        return Action(q_values.argmax().item())\n",
    "\n",
    "    def train(\n",
    "        self, state: State, action: Action, reward: float, next_state: State, done: bool\n",
    "    ):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        state_array = state.to_tensor()\n",
    "        next_state_array = next_state.to_tensor()\n",
    "\n",
    "        # Store in replay buffer\n",
    "        self.memory.push(state_array, action.value, reward, next_state_array, done)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # Update epsilon (linear decay)\n",
    "        if self.steps > self.learning_starts:\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "        # Don't train until warmup complete\n",
    "        if self.steps < self.learning_starts:\n",
    "            return\n",
    "\n",
    "        # Don't train if buffer too small\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Perform training update\n",
    "        loss = self._update_network()\n",
    "        if loss is not None:\n",
    "            self.losses.append(loss)\n",
    "\n",
    "        # Update target network\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            print(f\"üîÑ Updated target network at step {self.steps:,}\")\n",
    "\n",
    "        # Track episodes\n",
    "        if done:\n",
    "            self.episodes_trained += 1\n",
    "\n",
    "    def _update_network(self):\n",
    "        \"\"\"Update Q-network using batch from replay buffer.\"\"\"\n",
    "        # Sample batch\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        if batch is None:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones, indices, weights = batch\n",
    "        weights_t = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_t = torch.FloatTensor(states).to(device)\n",
    "        actions_t = torch.LongTensor(actions).to(device)\n",
    "        rewards_t = torch.FloatTensor(rewards).to(device)\n",
    "        next_states_t = torch.FloatTensor(next_states).to(device)\n",
    "        dones_t = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        # Current Q-values\n",
    "        current_q = (\n",
    "            self.q_network(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "        )\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            # Double DQN: use online network to select, target to evaluate\n",
    "            next_actions = self.q_network(next_states_t).argmax(1)\n",
    "            next_q = (\n",
    "                self.target_network(next_states_t)\n",
    "                .gather(1, next_actions.unsqueeze(1))\n",
    "                .squeeze(1)\n",
    "            )\n",
    "\n",
    "            target_q = rewards_t + (1 - dones_t) * self.discount_factor * next_q\n",
    "\n",
    "        # Compute loss (element-wise for PER)\n",
    "        td_errors = current_q - target_q\n",
    "        losses = self.criterion(current_q, target_q)\n",
    "\n",
    "        # Apply importance sampling weights\n",
    "        loss = (losses * weights_t).mean()\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities\n",
    "        self.memory.update_priorities(indices, td_errors.detach().cpu().numpy())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _state_to_tensor(self, state: State) -> torch.Tensor:\n",
    "        \"\"\"Convert State to tensor.\"\"\"\n",
    "        state_array = state.to_tensor()\n",
    "        return torch.FloatTensor(state_array).unsqueeze(0).to(device)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"q_network_state\": self.q_network.state_dict(),\n",
    "                \"target_network_state\": self.target_network.state_dict(),\n",
    "                \"optimizer_state\": self.optimizer.state_dict(),\n",
    "                \"grid_size\": self.grid_size,\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"steps\": self.steps,\n",
    "                \"episodes_trained\": self.episodes_trained,\n",
    "            },\n",
    "            filepath,\n",
    "        )\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "        print(f\"   Episodes: {self.episodes_trained}\")\n",
    "        print(f\"   Steps: {self.steps:,}\")\n",
    "        print(f\"   Epsilon: {self.epsilon:.4f}\")\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Load model and training state.\"\"\"\n",
    "        if not Path(filepath).exists():\n",
    "            print(f\"‚ö†Ô∏è  No saved model found at {filepath}\")\n",
    "            return\n",
    "\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "\n",
    "        self.q_network.load_state_dict(checkpoint[\"q_network_state\"])\n",
    "        self.target_network.load_state_dict(checkpoint[\"target_network_state\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        self.grid_size = checkpoint[\"grid_size\"]\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "        self.steps = checkpoint[\"steps\"]\n",
    "        self.episodes_trained = checkpoint[\"episodes_trained\"]\n",
    "\n",
    "        print(f\"‚úÖ Model loaded from {filepath}\")\n",
    "        print(f\"   Episodes: {self.episodes_trained}\")\n",
    "        print(f\"   Steps: {self.steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "GRID_SIZE = 10\n",
    "EPISODES = 3_000\n",
    "\n",
    "# Agent hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.01\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100_000\n",
    "TARGET_UPDATE = 1_000\n",
    "\n",
    "# Save locations\n",
    "MODEL_PATH = \"models/dqn_cnn.pkl\"\n",
    "RESULTS_DIR = f\"results/dqn_{GRID_SIZE}x{GRID_SIZE}\"\n",
    "\n",
    "print(f\"üéÆ Training DQN on {GRID_SIZE}√ó{GRID_SIZE} grid\")\n",
    "print(f\"üìà Episodes: {EPISODES:,}\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "config = GameConfig(grid_size=GRID_SIZE)\n",
    "game = SnakeGameEngine(config)\n",
    "\n",
    "# Agent\n",
    "agent = DQNAgent(grid_size=GRID_SIZE)\n",
    "\n",
    "# Metrics\n",
    "metrics = TrainingMetrics(save_dir=RESULTS_DIR)\n",
    "\n",
    "print(\"‚úÖ Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score = 0\n",
    "pbar = tqdm(range(1, EPISODES + 1), desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    for episode in pbar:\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            reward, done, score = game.step(action)\n",
    "            next_state = game.get_state()\n",
    "\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        metrics.record_episode(episode, score, steps, episode_reward)\n",
    "\n",
    "        if score > record_score:\n",
    "            record_score = score\n",
    "\n",
    "        # Update progress\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Avg\": f\"{metrics.get_recent_average_score():.2f}\",\n",
    "                \"Best\": record_score,\n",
    "                \"Œµ\": f\"{agent.epsilon:.3f}\",\n",
    "                \"Buffer\": len(agent.memory),\n",
    "            }\n",
    "        )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot(show=True, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent.losses:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Raw loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(agent.losses, alpha=0.3)\n",
    "    if len(agent.losses) > 1000:\n",
    "        window = 1000\n",
    "        smoothed = np.convolve(agent.losses, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smoothed, linewidth=2, label=f'Moving Avg ({window})')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(agent.losses, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Loss Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Loss Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(agent.losses):.4f}\")\n",
    "    print(f\"  Std: {np.std(agent.losses):.4f}\")\n",
    "    print(f\"  Min: {np.min(agent.losses):.4f}\")\n",
    "    print(f\"  Max: {np.max(agent.losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0.0  # Pure exploitation\n",
    "\n",
    "test_episodes = 10\n",
    "test_scores = []\n",
    "\n",
    "print(f\"Testing for {test_episodes} episodes...\")\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = game.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = game.step(action)\n",
    "        state = game.get_state()\n",
    "    \n",
    "    test_scores.append(score)\n",
    "    print(f\"  Episode {ep+1}: Score = {score}\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Average: {np.mean(test_scores):.2f}\")\n",
    "print(f\"  Best: {max(test_scores)}\")\n",
    "print(f\"  Worst: {min(test_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export for Google Colab (Optional)\n",
    "\n",
    "If running on Google Colab, you can download the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download model in Colab\n",
    "# from google.colab import files\n",
    "# files.download(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "rl-snake (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
