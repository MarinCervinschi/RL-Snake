{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO) Training Notebook\n",
    "\n",
    "This notebook trains a **PPO** agent on the Snake game.\n",
    "\n",
    "**Grid Size:** 20√ó20 (optimal for PPO)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Episodes to convergence: 4000-6000\n",
    "- Final average score: 30-50 apples\n",
    "- Training time: 25-35 minutes (CPU) / 10-15 minutes (GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# !git clone https://github.com/MarinCervinschi/rl-snake.git\n",
    "# %cd rl-snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from game.config import GameConfig\n",
    "from game.engine import SnakeGameEngine\n",
    "from game.entities import Action, State\n",
    "from utils.metrics import TrainingMetrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"Actor-Critic network with shared CNN backbone.\"\"\"\n",
    "\n",
    "    def __init__(self, grid_size: int, num_actions: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared CNN\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.Tanh(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.Tanh(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.flat_size = 64 * grid_size * grid_size\n",
    "        self.shared_fc = nn.Sequential(nn.Linear(self.flat_size, 512), nn.Tanh())\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.Tanh(),\n",
    "            nn.Linear(256, num_actions),\n",
    "        )\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.Tanh(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_features = self.shared_conv(x)\n",
    "        flat = conv_features.view(conv_features.size(0), -1)\n",
    "        shared = self.shared_fc(flat)\n",
    "        \n",
    "        action_logits = self.actor(shared)\n",
    "        action_probs = torch.softmax(action_logits, dim=-1)\n",
    "        state_values = self.critic(shared)\n",
    "        \n",
    "        return action_probs, state_values\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_probs, value = self.forward(x)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        return action, dist.log_prob(action), dist.entropy(), value\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"Buffer for storing rollout experience.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def push(self, state, action, log_prob, reward, value, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def get(self):\n",
    "        return (\n",
    "            np.array(self.states, dtype=np.float32),\n",
    "            np.array(self.actions, dtype=np.int64),\n",
    "            np.array(self.log_probs, dtype=np.float32),\n",
    "            np.array(self.rewards, dtype=np.float32),\n",
    "            np.array(self.values, dtype=np.float32),\n",
    "            np.array(self.dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.values.clear()\n",
    "        self.dones.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int = 20,\n",
    "        learning_rate: float = 0.0003,\n",
    "        discount_factor: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        value_coef: float = 0.5,\n",
    "        entropy_coef: float = 0.01,\n",
    "        rollout_length: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        epochs: int = 4,\n",
    "        max_grad_norm: float = 0.5,\n",
    "    ):\n",
    "        self.grid_size = grid_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.rollout_length = rollout_length\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.device = device\n",
    "        self.policy = ActorCriticNetwork(grid_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "        self.steps = 0\n",
    "        self.updates = 0\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "\n",
    "    def get_action(self, state: State) -> Action:\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = self.policy(state_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        return Action(dist.sample().item())\n",
    "\n",
    "    def train(self, state: State, action: Action, reward: float, next_state: State, done: bool):\n",
    "        state_array = state.to_tensor()\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        action_tensor = torch.tensor([action.value], device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, log_prob, _, value = self.policy.get_action_and_value(state_tensor, action_tensor)\n",
    "\n",
    "        self.buffer.push(state_array, action.value, log_prob.item(), reward, value.item(), done)\n",
    "        self.steps += 1\n",
    "\n",
    "        if len(self.buffer) >= self.rollout_length:\n",
    "            self._update()\n",
    "            self.buffer.clear()\n",
    "\n",
    "    def _update(self):\n",
    "        states, actions, old_log_probs, rewards, values, dones = self.buffer.get()\n",
    "\n",
    "        # Compute advantages using GAE\n",
    "        advantages = self._compute_gae(rewards, values, dones)\n",
    "        returns = advantages + values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_t = torch.FloatTensor(states).to(self.device)\n",
    "        actions_t = torch.LongTensor(actions).to(self.device)\n",
    "        old_log_probs_t = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        advantages_t = torch.FloatTensor(advantages).to(self.device)\n",
    "        returns_t = torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "        # Train for multiple epochs\n",
    "        for _ in range(self.epochs):\n",
    "            indices = np.arange(len(states))\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "\n",
    "                batch_states = states_t[batch_idx]\n",
    "                batch_actions = actions_t[batch_idx]\n",
    "                batch_old_log_probs = old_log_probs_t[batch_idx]\n",
    "                batch_advantages = advantages_t[batch_idx]\n",
    "                batch_returns = returns_t[batch_idx]\n",
    "\n",
    "                _, new_log_probs, entropy, values_pred = self.policy.get_action_and_value(\n",
    "                    batch_states, batch_actions\n",
    "                )\n",
    "\n",
    "                # PPO clipped objective\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                values_pred = values_pred.squeeze()\n",
    "                value_loss = ((values_pred - batch_returns) ** 2).mean()\n",
    "\n",
    "                # Entropy loss\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "\n",
    "                # Optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.policy_losses.append(policy_loss.item())\n",
    "                self.value_losses.append(value_loss.item())\n",
    "                self.entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "        self.updates += 1\n",
    "\n",
    "    def _compute_gae(self, rewards, values, dones):\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        last_gae = 0\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_value = 0 if t == len(rewards) - 1 else values[t + 1]\n",
    "            delta = rewards[t] + self.discount_factor * next_value * (1 - dones[t]) - values[t]\n",
    "            last_gae = delta + self.discount_factor * self.gae_lambda * (1 - dones[t]) * last_gae\n",
    "            advantages[t] = last_gae\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def _state_to_tensor(self, state: State):\n",
    "        state_array = state.to_tensor()\n",
    "        return torch.FloatTensor(state_array).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"policy_state\": self.policy.state_dict(),\n",
    "                \"optimizer_state\": self.optimizer.state_dict(),\n",
    "                \"steps\": self.steps,\n",
    "                \"updates\": self.updates,\n",
    "            },\n",
    "            filepath,\n",
    "        )\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        if not Path(filepath).exists():\n",
    "            print(f\"‚ö†Ô∏è  No saved model found\")\n",
    "            return\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.policy.load_state_dict(checkpoint[\"policy_state\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        self.steps = checkpoint[\"steps\"]\n",
    "        self.updates = checkpoint[\"updates\"]\n",
    "        print(f\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "GRID_SIZE = 20\n",
    "EPISODES = 5_000\n",
    "\n",
    "# Agent hyperparameters\n",
    "LEARNING_RATE = 0.0003\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "ROLLOUT_LENGTH = 2048\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "\n",
    "# Save locations\n",
    "MODEL_PATH = \"models/ppo.pkl\"\n",
    "RESULTS_DIR = f\"results/ppo_{GRID_SIZE}x{GRID_SIZE}\"\n",
    "\n",
    "print(f\"üéÆ Training PPO on {GRID_SIZE}√ó{GRID_SIZE} grid\")\n",
    "print(f\"üìà Episodes: {EPISODES:,}\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GameConfig(grid_size=GRID_SIZE)\n",
    "game = SnakeGameEngine(config)\n",
    "\n",
    "agent = PPOAgent(\n",
    "    grid_size=GRID_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    gae_lambda=GAE_LAMBDA,\n",
    "    clip_epsilon=CLIP_EPSILON,\n",
    "    value_coef=VALUE_COEF,\n",
    "    entropy_coef=ENTROPY_COEF,\n",
    "    rollout_length=ROLLOUT_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "metrics = TrainingMetrics(save_dir=RESULTS_DIR)\n",
    "print(\"‚úÖ Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score = 0\n",
    "pbar = tqdm(range(1, EPISODES + 1), desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    for episode in pbar:\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            reward, done, score = game.step(action)\n",
    "            next_state = game.get_state()\n",
    "\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        metrics.record_episode(episode, score, steps, episode_reward)\n",
    "\n",
    "        if score > record_score:\n",
    "            record_score = score\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Avg\": f\"{metrics.get_recent_average_score():.2f}\",\n",
    "                \"Best\": record_score,\n",
    "                \"Updates\": agent.updates,\n",
    "            }\n",
    "        )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(MODEL_PATH)\n",
    "metrics.print_summary()\n",
    "metrics.plot(show=True, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent.policy_losses:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(agent.policy_losses, alpha=0.3)\n",
    "    axes[0].set_title('Policy Loss')\n",
    "    axes[0].set_xlabel('Update Step')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(agent.value_losses, alpha=0.3, color='orange')\n",
    "    axes[1].set_title('Value Loss')\n",
    "    axes[1].set_xlabel('Update Step')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(agent.entropy_losses, alpha=0.3, color='green')\n",
    "    axes[2].set_title('Entropy Loss')\n",
    "    axes[2].set_xlabel('Update Step')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "test_scores = []\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = game.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = game.step(action)\n",
    "        state = game.get_state()\n",
    "    \n",
    "    test_scores.append(score)\n",
    "    print(f\"Episode {ep+1}: {score}\")\n",
    "\n",
    "print(f\"\\nAverage: {np.mean(test_scores):.2f}\")\n",
    "print(f\"Best: {max(test_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export for Google Colab (Optional)\n",
    "\n",
    "If running on Google Colab, you can download the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download model in Colab\n",
    "# from google.colab import files\n",
    "# files.download(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "rl-snake (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
