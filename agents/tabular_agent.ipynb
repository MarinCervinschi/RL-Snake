{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning Training Notebook\n",
    "\n",
    "This notebook trains a **Tabular Q-Learning** agent on the Snake game.\n",
    "\n",
    "**Grid Size:** 5√ó5 (optimal for tabular methods)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Episodes to convergence: 500-1000\n",
    "- Final average score: 8-12 apples\n",
    "- Training time: 1-2 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# !git clone https://github.com/MarinCervinschi/rl-snake.git\n",
    "# %cd rl-snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from game.config import GameConfig\n",
    "from game.engine import SnakeGameEngine\n",
    "from utils.metrics import TrainingMetrics\n",
    "\n",
    "# For Google Colab: uncomment the following\n",
    "# !git clone https://github.com/YOUR_REPO/rl-snake.git\n",
    "# %cd rl-snake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabular Q-Learning Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "from game.entities import Action, State\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning with dictionary-based Q-table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_size: int = 5,\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.99,\n",
    "        epsilon: float = 1.0,\n",
    "        epsilon_decay: float = 0.995,\n",
    "        min_epsilon: float = 0.01,\n",
    "    ):\n",
    "        self.grid_size = grid_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        # Q-table: dictionary mapping state_key -> [Q(s,a) for each action]\n",
    "        self.q_table: Dict[Tuple, np.ndarray] = {}\n",
    "        self.updates_performed = 0\n",
    "\n",
    "    def get_action(self, state: State) -> Action:\n",
    "        \"\"\"Select action using Œµ-greedy policy.\"\"\"\n",
    "        state_key = self._state_to_key(state)\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(list(Action))\n",
    "        else:\n",
    "            q_values = self.q_table[state_key]\n",
    "            action_idx = np.argmax(q_values)\n",
    "            return Action(action_idx)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        state: State,\n",
    "        action: Action,\n",
    "        reward: float,\n",
    "        next_state: State,\n",
    "        done: bool,\n",
    "    ) -> None:\n",
    "        \"\"\"Update Q-table using Bellman equation.\"\"\"\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "        action_idx = action.value\n",
    "\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "        if next_state_key not in self.q_table:\n",
    "            self.q_table[next_state_key] = np.zeros(4, dtype=np.float32)\n",
    "\n",
    "        current_q = self.q_table[state_key][action_idx]\n",
    "\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            max_next_q = np.max(self.q_table[next_state_key])\n",
    "            target_q = reward + self.discount_factor * max_next_q\n",
    "\n",
    "        self.q_table[state_key][action_idx] = current_q + self.learning_rate * (\n",
    "            target_q - current_q\n",
    "        )\n",
    "\n",
    "        self.updates_performed += 1\n",
    "\n",
    "        if done and self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def _state_to_key(self, state: State) -> Tuple:\n",
    "        \"\"\"Convert State to hashable key for dictionary.\"\"\"\n",
    "        return state.to_position_tuple()\n",
    "\n",
    "    def save(self, filepath: str = \"models/tabular_q_learning.pkl\") -> None:\n",
    "        \"\"\"Save Q-table and agent state.\"\"\"\n",
    "        save_path = Path(filepath)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        save_dict = {\n",
    "            \"q_table\": self.q_table,\n",
    "            \"grid_size\": self.grid_size,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"updates_performed\": self.updates_performed,\n",
    "        }\n",
    "\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "        print(f\"   Q-table size: {len(self.q_table):,} entries\")\n",
    "\n",
    "    def load(self, filepath: str = \"models/tabular_q_learning.pkl\") -> None:\n",
    "        \"\"\"Load Q-table and agent state.\"\"\"\n",
    "        load_path = Path(filepath)\n",
    "\n",
    "        if not load_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  No saved model found at {filepath}\")\n",
    "            return\n",
    "\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            save_dict = pickle.load(f)\n",
    "\n",
    "        self.q_table = save_dict[\"q_table\"]\n",
    "        self.grid_size = save_dict[\"grid_size\"]\n",
    "        self.epsilon = save_dict[\"epsilon\"]\n",
    "        self.learning_rate = save_dict[\"learning_rate\"]\n",
    "        self.discount_factor = save_dict[\"discount_factor\"]\n",
    "        self.updates_performed = save_dict.get(\"updates_performed\", 0)\n",
    "\n",
    "        print(f\"‚úÖ Model loaded from {filepath}\")\n",
    "        print(f\"   States in Q-table: {len(self.q_table):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "GRID_SIZE = 5\n",
    "EPISODES = 10_000\n",
    "\n",
    "# Agent hyperparameters\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "# Save location\n",
    "MODEL_PATH = \"models/tabular_q_learning.pkl\"\n",
    "RESULTS_DIR = f\"results/tabular_{GRID_SIZE}x{GRID_SIZE}\"\n",
    "\n",
    "print(f\"üéÆ Training Tabular Q-Learning on {GRID_SIZE}√ó{GRID_SIZE} grid\")\n",
    "print(f\"üìà Episodes: {EPISODES:,}\")\n",
    "print(f\"üß† Hyperparameters: Œ±={LEARNING_RATE}, Œ≥={DISCOUNT_FACTOR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create game environment\n",
    "config = GameConfig(grid_size=GRID_SIZE)\n",
    "game = SnakeGameEngine(config)\n",
    "\n",
    "# Create agent\n",
    "agent = QLearningAgent(\n",
    "    grid_size=GRID_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    epsilon=EPSILON_START,\n",
    "    epsilon_decay=EPSILON_DECAY,\n",
    "    min_epsilon=EPSILON_MIN,\n",
    ")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics = TrainingMetrics(save_dir=RESULTS_DIR)\n",
    "\n",
    "print(\"‚úÖ Environment and agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_score = 0\n",
    "pbar = tqdm(range(1, EPISODES + 1), desc=\"Training\")\n",
    "\n",
    "try:\n",
    "    for episode in pbar:\n",
    "        # Reset environment\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        # Episode loop\n",
    "        while not done:\n",
    "            # Agent selects action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Execute action in environment\n",
    "            reward, done, score = game.step(action)\n",
    "            next_state = game.get_state()\n",
    "\n",
    "            # Agent learns from transition\n",
    "            agent.train(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update tracking\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        # Record episode metrics\n",
    "        metrics.record_episode(episode, score, steps, episode_reward)\n",
    "\n",
    "        # Update record\n",
    "        if score > record_score:\n",
    "            record_score = score\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Avg Score\": f\"{metrics.get_recent_average_score():.2f}\",\n",
    "                \"Best\": record_score,\n",
    "                \"Œµ\": f\"{agent.epsilon:.3f}\",\n",
    "                \"Q-table\": len(agent.q_table),\n",
    "            }\n",
    "        )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(MODEL_PATH)\n",
    "print(f\"\\n‚úÖ Model saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "metrics.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display plots\n",
    "metrics.plot(show=True, save=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table statistics\n",
    "print(f\"Q-table Statistics:\")\n",
    "print(f\"  Total states visited: {len(agent.q_table):,}\")\n",
    "print(f\"  Updates performed: {agent.updates_performed:,}\")\n",
    "print(f\"  Final epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "# Q-value distribution\n",
    "all_q_values = [q for q_array in agent.q_table.values() for q in q_array]\n",
    "print(f\"\\nQ-value Distribution:\")\n",
    "print(f\"  Mean: {np.mean(all_q_values):.2f}\")\n",
    "print(f\"  Std: {np.std(all_q_values):.2f}\")\n",
    "print(f\"  Min: {np.min(all_q_values):.2f}\")\n",
    "print(f\"  Max: {np.max(all_q_values):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-value distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(all_q_values, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel('Q-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Q-value Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "max_q_per_state = [np.max(q_array) for q_array in agent.q_table.values()]\n",
    "plt.hist(max_q_per_state, bins=50, alpha=0.7, color='green')\n",
    "plt.xlabel('Max Q-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Max Q-value per State Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epsilon to 0 for pure exploitation\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "# Run test episodes\n",
    "test_episodes = 10\n",
    "test_scores = []\n",
    "\n",
    "print(f\"Testing agent for {test_episodes} episodes...\")\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state = game.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = game.step(action)\n",
    "        state = game.get_state()\n",
    "    \n",
    "    test_scores.append(score)\n",
    "    print(f\"  Episode {ep+1}: Score = {score}\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Average Score: {np.mean(test_scores):.2f}\")\n",
    "print(f\"  Best Score: {max(test_scores)}\")\n",
    "print(f\"  Worst Score: {min(test_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export for Google Colab (Optional)\n",
    "\n",
    "If running on Google Colab, you can download the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download model in Colab\n",
    "# from google.colab import files\n",
    "# files.download(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-snake (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
